{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e2be53-cc0c-4956-ade0-e40e28da5864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using project ID: predictive-behavior-analytics\n",
      "INFO:__main__:Executing BigQuery and loading data...\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723995332.202424 1987039 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "INFO:__main__:Data loaded. Shape: (2500, 16)\n",
      "INFO:__main__:\n",
      "DataFrame info:\n",
      "INFO:__main__:\n",
      "Memory usage: 6.97 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2500 entries, 0 to 2499\n",
      "Data columns (total 16 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   visitorId             0 non-null      Int64 \n",
      " 1   visitNumber           2500 non-null   Int64 \n",
      " 2   visitId               2500 non-null   Int64 \n",
      " 3   visitStartTime        2500 non-null   Int64 \n",
      " 4   date                  2500 non-null   object\n",
      " 5   totals                2500 non-null   object\n",
      " 6   trafficSource         2500 non-null   object\n",
      " 7   device                2500 non-null   object\n",
      " 8   geoNetwork            2500 non-null   object\n",
      " 9   customDimensions      2500 non-null   object\n",
      " 10  hits                  2500 non-null   object\n",
      " 11  fullVisitorId         2500 non-null   object\n",
      " 12  userId                0 non-null      object\n",
      " 13  clientId              0 non-null      object\n",
      " 14  channelGrouping       2500 non-null   object\n",
      " 15  socialEngagementType  2500 non-null   object\n",
      "dtypes: Int64(4), object(12)\n",
      "memory usage: 322.4+ KB\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set up credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '/Users/ttanaka/Downloads/predictive-behavior-analytics-b509bad93e58.json'\n",
    ")\n",
    "\n",
    "# Set the correct project ID\n",
    "project_id = \"predictive-behavior-analytics\"\n",
    "\n",
    "# Log the project ID being used\n",
    "logger.info(f\"Using project ID: {project_id}\")\n",
    "\n",
    "# Create a BigQuery client\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "# Your BigQuery query\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  `bigquery-public-data.google_analytics_sample.ga_sessions_20170801`\n",
    "LIMIT 2500\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and load data into a DataFrame\n",
    "logger.info(\"Executing BigQuery and loading data...\")\n",
    "df = client.query(query).to_dataframe()\n",
    "logger.info(f\"Data loaded. Shape: {df.shape}\")\n",
    "\n",
    "import json\n",
    "\n",
    "# Print information about the DataFrame\n",
    "logger.info(\"\\nDataFrame info:\")\n",
    "df.info()\n",
    "\n",
    "# Print memory usage\n",
    "logger.info(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dbe6821-889f-4157-a05c-fc90d488cbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using project ID: predictive-behavior-analytics\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "import logging\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set up credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '/Users/ttanaka/Downloads/predictive-behavior-analytics-b509bad93e58.json'\n",
    ")\n",
    "\n",
    "# Set the correct project ID\n",
    "project_id = \"predictive-behavior-analytics\"\n",
    "\n",
    "# Log the project ID being used\n",
    "logger.info(f\"Using project ID: {project_id}\")\n",
    "\n",
    "# Create a BigQuery client\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "# Your BigQuery query\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  `bigquery-public-data.google_analytics_sample.ga_sessions_20170801`\n",
    "LIMIT 10000\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95ad5b77-0ed1-4779-b7f0-1ab3a4763c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning functions\n",
    "def safe_json_loads(x):\n",
    "    try:\n",
    "        return json.loads(x) if isinstance(x, str) else x\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "def flatten_nested_columns(df):\n",
    "    nested_columns = ['totals', 'trafficSource', 'device', 'geoNetwork']\n",
    "    for col in nested_columns:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                flattened = pd.json_normalize(df[col].apply(safe_json_loads))\n",
    "                flattened.columns = [f'{col}_{subcol}' for subcol in flattened.columns]\n",
    "                df = pd.concat([df.drop(col, axis=1), flattened], axis=1)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error flattening column {col}: {str(e)}\")\n",
    "    return df\n",
    "\n",
    "def extract_hit_level_data(df):\n",
    "    try:\n",
    "        hits_data = df.apply(lambda row: pd.json_normalize(safe_json_loads(row['hits'])), axis=1).explode().reset_index()\n",
    "        hits_data = hits_data.rename(columns={'index': 'session_id'})\n",
    "        \n",
    "        for col in ['fullVisitorId', 'visitId', 'date']:\n",
    "            hits_data[col] = df[col].repeat(df['hits'].apply(lambda x: len(safe_json_loads(x)))).reset_index(drop=True)\n",
    "        \n",
    "        hits_data['date'] = pd.to_datetime(hits_data['date'], format='%Y%m%d')\n",
    "        \n",
    "        columns_to_keep = [\n",
    "            'session_id', 'fullVisitorId', 'visitId', 'date',\n",
    "            'hitNumber', 'time', 'hour', 'minute',\n",
    "            'isEntrance', 'isExit', 'page.pagePath', 'page.pageTitle',\n",
    "            'eventInfo.eventCategory', 'eventInfo.eventAction', 'eventInfo.eventLabel',\n",
    "            'transaction.transactionId', 'transaction.transactionRevenue',\n",
    "            'item.productName', 'item.productCategory', 'item.productSKU', 'item.itemRevenue'\n",
    "        ]\n",
    "        \n",
    "        hits_df = hits_data[columns_to_keep].copy()\n",
    "        \n",
    "        numeric_columns = ['time', 'hour', 'minute', 'transaction.transactionRevenue', 'item.itemRevenue']\n",
    "        for col in numeric_columns:\n",
    "            if col in hits_df.columns:\n",
    "                hits_df[col] = pd.to_numeric(hits_df[col], errors='coerce')\n",
    "        \n",
    "        boolean_columns = ['isEntrance', 'isExit']\n",
    "        for col in boolean_columns:\n",
    "            if col in hits_df.columns:\n",
    "                hits_df[col] = hits_df[col].astype(bool)\n",
    "        \n",
    "        return hits_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in extract_hit_level_data: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def clean_data(df):\n",
    "    df_cleaned = flatten_nested_columns(df)\n",
    "    logger.info(f\"Flattened DataFrame shape: {df_cleaned.shape}\")\n",
    "\n",
    "    if 'visitorId' in df_cleaned.columns and df_cleaned['visitorId'].isna().all():\n",
    "        df_cleaned = df_cleaned.drop('visitorId', axis=1)\n",
    "        logger.info(\"Dropped empty 'visitorId' column\")\n",
    "\n",
    "    df_cleaned['date'] = pd.to_datetime(df_cleaned['date'], format='%Y%m%d')\n",
    "\n",
    "    if 'geoNetwork_country' in df_cleaned.columns:\n",
    "        df_cleaned['country'] = df_cleaned['geoNetwork_country']\n",
    "\n",
    "    numeric_columns = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "    categorical_columns = df_cleaned.select_dtypes(exclude=[np.number, 'datetime64']).columns\n",
    "\n",
    "    logger.info(f\"Number of numeric columns: {len(numeric_columns)}\")\n",
    "    logger.info(f\"Number of categorical columns: {len(categorical_columns)}\")\n",
    "\n",
    "    for col in df_cleaned.columns:\n",
    "        if col in numeric_columns:\n",
    "            df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
    "        elif col in categorical_columns:\n",
    "            df_cleaned[col] = df_cleaned[col].astype(str)\n",
    "\n",
    "    all_nan_columns = df_cleaned.columns[df_cleaned.isna().all()].tolist()\n",
    "    if all_nan_columns:\n",
    "        logger.warning(f\"Columns with all NaN values: {all_nan_columns}\")\n",
    "        df_cleaned = df_cleaned.drop(columns=all_nan_columns)\n",
    "        numeric_columns = [col for col in numeric_columns if col not in all_nan_columns]\n",
    "        categorical_columns = [col for col in categorical_columns if col not in all_nan_columns]\n",
    "\n",
    "    numeric_imputer = SimpleImputer(strategy='median')\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Shape of numeric data before imputation: {df_cleaned[numeric_columns].shape}\")\n",
    "        imputed_numeric = numeric_imputer.fit_transform(df_cleaned[numeric_columns])\n",
    "        logger.info(f\"Shape of imputed numeric data: {imputed_numeric.shape}\")\n",
    "        df_cleaned[numeric_columns] = imputed_numeric\n",
    "        logger.info(\"Numeric imputation successful\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during numeric imputation: {str(e)}\")\n",
    "        logger.info(\"Numeric columns:\")\n",
    "        logger.info(numeric_columns.tolist())\n",
    "\n",
    "    try:\n",
    "        df_cleaned[categorical_columns] = categorical_imputer.fit_transform(df_cleaned[categorical_columns])\n",
    "        logger.info(\"Categorical imputation successful\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during categorical imputation: {str(e)}\")\n",
    "        logger.info(\"Categorical columns:\")\n",
    "        logger.info(categorical_columns.tolist())\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_cleaned[numeric_columns] = scaler.fit_transform(df_cleaned[numeric_columns])\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    for col in categorical_columns:\n",
    "        df_cleaned[col] = le.fit_transform(df_cleaned[col].astype(str))\n",
    "\n",
    "    logger.info(f\"Final cleaned session-level DataFrame shape: {df_cleaned.shape}\")\n",
    "\n",
    "    hit_level_df = extract_hit_level_data(df)\n",
    "    logger.info(f\"Hit-level DataFrame shape: {hit_level_df.shape}\")\n",
    "\n",
    "    return df_cleaned, hit_level_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2407ceed-0e81-4e1e-8feb-a9e9f36453f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering function (corrected)\n",
    "def engineer_features(df_cleaned, hit_level_df):\n",
    "    logger.info(\"Starting feature engineering\")\n",
    "    \n",
    "    required_columns = ['date', 'totals_timeOnSite', 'totals_pageviews', 'totals_bounces', 'device_deviceCategory', 'trafficSource_medium', 'geoNetwork_country', 'totals_visits']\n",
    "    if not all(col in df_cleaned.columns for col in required_columns):\n",
    "        missing_cols = [col for col in required_columns if col not in df_cleaned.columns]\n",
    "        logger.error(f\"Missing required columns in df_cleaned: {missing_cols}\")\n",
    "        raise ValueError(f\"Missing required columns in df_cleaned: {missing_cols}\")\n",
    "\n",
    "    df_engineered = df_cleaned.copy()\n",
    "    \n",
    "    try:\n",
    "        df_engineered['day_of_week'] = df_engineered['date'].dt.dayofweek\n",
    "        df_engineered['is_weekend'] = df_engineered['day_of_week'].isin([5, 6]).astype(int)\n",
    "        df_engineered['month'] = df_engineered['date'].dt.month\n",
    "        df_engineered['quarter'] = df_engineered['date'].dt.quarter\n",
    "        \n",
    "        df_engineered['session_duration_seconds'] = df_engineered['totals_timeOnSite']\n",
    "        df_engineered['pageviews_per_session'] = df_engineered['totals_pageviews']\n",
    "        df_engineered['is_bounce'] = (df_engineered['totals_bounces'] > 0).astype(int)\n",
    "        \n",
    "        df_engineered['is_mobile'] = (df_engineered['device_deviceCategory'] == 'mobile').astype(int)\n",
    "        df_engineered['is_tablet'] = (df_engineered['device_deviceCategory'] == 'tablet').astype(int)\n",
    "        df_engineered['is_desktop'] = (df_engineered['device_deviceCategory'] == 'desktop').astype(int)\n",
    "        \n",
    "        df_engineered['is_organic_search'] = (df_engineered['trafficSource_medium'] == 'organic').astype(int)\n",
    "        df_engineered['is_paid_search'] = (df_engineered['trafficSource_medium'] == 'cpc').astype(int)\n",
    "        df_engineered['is_referral'] = (df_engineered['trafficSource_medium'] == 'referral').astype(int)\n",
    "        \n",
    "        df_engineered['is_us'] = (df_engineered['geoNetwork_country'] == 'United States').astype(int)\n",
    "        \n",
    "        logger.info(\"Processing hit-level data\")\n",
    "        hit_level_features = hit_level_df.groupby('fullVisitorId').agg({\n",
    "            'time': ['count', 'mean', 'max'],\n",
    "            'isEntrance': 'sum',\n",
    "            'isExit': 'sum',\n",
    "            'eventInfo.eventCategory': 'nunique',\n",
    "            'transaction.transactionId': 'nunique',\n",
    "            'transaction.transactionRevenue': 'sum',\n",
    "            'item.productName': 'nunique',\n",
    "        })\n",
    "        \n",
    "        hit_level_features.columns = [\n",
    "            'total_hits', 'avg_time_per_hit', 'max_time_per_hit',\n",
    "            'num_entrance_pages', 'num_exit_pages', 'num_unique_events',\n",
    "            'num_transactions', 'total_revenue', 'num_unique_products_viewed'\n",
    "        ]\n",
    "        \n",
    "        df_engineered = df_engineered.merge(hit_level_features, left_on='fullVisitorId', right_index=True, how='left')\n",
    "        \n",
    "        df_engineered['avg_pageviews_per_session'] = df_engineered['pageviews_per_session'] / df_engineered['totals_visits'].replace(0, 1)\n",
    "        df_engineered['conversion_rate'] = df_engineered['num_transactions'] / df_engineered['totals_visits'].replace(0, 1)\n",
    "        df_engineered['avg_revenue_per_session'] = df_engineered['total_revenue'] / df_engineered['totals_visits'].replace(0, 1)\n",
    "        \n",
    "        df_engineered['user_value_segment'] = pd.qcut(df_engineered['total_revenue'].fillna(0), q=4, labels=['Low', 'Medium', 'High', 'VIP'])\n",
    "        df_engineered['engagement_segment'] = pd.qcut(df_engineered['total_hits'].fillna(0), q=3, labels=['Low', 'Medium', 'High'])\n",
    "        \n",
    "        logger.info(\"Feature engineering completed successfully\")\n",
    "        \n",
    "        # Save the engineered DataFrame to a CSV file\n",
    "        save_path = \"df_engineered.csv\"\n",
    "        df_engineered.to_csv(save_path, index=False)\n",
    "        logger.info(f\"Engineered DataFrame saved to {save_path}\")\n",
    "        \n",
    "        return df_engineered\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during feature engineering: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Usage example:\n",
    "# df_cleaned, hit_level_df = clean_data(df)  # Assuming these functions are already defined\n",
    "# df_engineered = engineer_features(df_cleaned, hit_level_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53b472a1-097e-4053-aac6-0cb9a1a09678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Failed to save the DataFrame: name 'df_engineered' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_engineered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_engineered.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mdf_engineered\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(save_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngineered DataFrame saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_engineered' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    save_path = \"df_engineered.csv\"\n",
    "    df_engineered.to_csv(save_path, index=False)\n",
    "    logger.info(f\"Engineered DataFrame saved to {save_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save the DataFrame: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d1b210-09e7-462b-ae04-97e7fec5c059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize data types\n",
    "df_cleaned = optimize_dtypes(df_cleaned)\n",
    "hit_level_df = optimize_dtypes(hit_level_df)\n",
    "\n",
    "# Log the final memory usage\n",
    "logger.info(f\"Optimized session-level DataFrame memory usage: {df_cleaned.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "logger.info(f\"Optimized hit-level DataFrame memory usage: {hit_level_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beace13-0dd4-4760-9d52-cc419bff54f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection function\n",
    "def correlation_analysis(df, threshold=0.8):\n",
    "    corr_matrix = df.corr(method='spearman')\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n",
    "    plt.title('Feature Correlation Heatmap')\n",
    "    plt.show()\n",
    "    \n",
    "    high_corr_vars = np.where(np.abs(corr_matrix) > threshold)\n",
    "    high_corr_vars = [(corr_matrix.index[x], corr_matrix.columns[y]) for x, y in zip(*high_corr_vars) if x != y and x < y]\n",
    "    \n",
    "    return high_corr_vars\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3b0ed-3bf9-4dc0-8111-939b1f8c4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "try:\n",
    "    # Execute the query\n",
    "    df = client.query(query).to_dataframe()\n",
    "    logger.info(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "    print(\"Original dataframe columns:\")\n",
    "    print(df.columns)\n",
    "\n",
    "    # Data Cleaning\n",
    "    df_cleaned, hit_level_df = clean_data(df)\n",
    "    print(\"\\nCleaned dataframe columns:\")\n",
    "    print(df_cleaned.columns)\n",
    "\n",
    "    # Feature Engineering\n",
    "    print(\"\\nStarting feature engineering...\")\n",
    "    df_engineered = engineer_features(df_cleaned, hit_level_df)\n",
    "    print(\"Feature engineering completed.\")\n",
    "    print(\"\\nEngineered dataframe columns:\")\n",
    "    print(df_engineered.columns)\n",
    "\n",
    "    # Feature Selection\n",
    "    print(\"\\nStarting correlation analysis...\")\n",
    "    high_corr_features = correlation_analysis(df_engineered)\n",
    "    print(\"Correlation analysis completed.\")\n",
    "    print(\"\\nHighly correlated feature pairs:\")\n",
    "    for feat1, feat2 in high_corr_features:\n",
    "        print(f\"{feat1} - {feat2}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the process: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Main execution\n",
    "try:\n",
    "    # Data Cleaning\n",
    "    df_cleaned, hit_level_df = clean_data(df)\n",
    "    print(\"\\nCleaned dataframe columns:\")\n",
    "    print(df_cleaned.columns)\n",
    "\n",
    "    # Feature Engineering\n",
    "    print(\"\\nStarting feature engineering...\")\n",
    "    df_engineered = engineer_features(df_cleaned, hit_level_df)\n",
    "    print(\"Feature engineering completed.\")\n",
    "    print(\"\\nEngineered dataframe columns:\")\n",
    "    print(df_engineered.columns)\n",
    "\n",
    "    # Feature Selection\n",
    "    print(\"\\nStarting correlation analysis...\")\n",
    "    high_corr_features = correlation_analysis(df_engineered)\n",
    "    print(\"Correlation analysis completed.\")\n",
    "    print(\"\\nHighly correlated feature pairs:\")\n",
    "    for feat1, feat2 in high_corr_features:\n",
    "        print(f\"{feat1} - {feat2}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the process: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f07fc-ce42-49bd-a0d9-ce7761949718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
