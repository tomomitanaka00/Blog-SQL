{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729eede2-c7be-4463-90ad-9d8d382efad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using project ID: predictive-behavior-analytics\n",
      "INFO:__main__:Executing BigQuery and loading data...\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724000928.014292 2043395 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set up credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '/Users/ttanaka/Downloads/predictive-behavior-analytics-b509bad93e58.json'\n",
    ")\n",
    "\n",
    "# Set the correct project ID\n",
    "project_id = \"predictive-behavior-analytics\"\n",
    "\n",
    "# Log the project ID being used\n",
    "logger.info(f\"Using project ID: {project_id}\")\n",
    "\n",
    "# Create a BigQuery client\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "# BigQuery query with selected features relevant for Sales Prediction\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  CONCAT(fullVisitorId, CAST(visitId AS STRING)) AS session_id,\n",
    "  date,\n",
    "  totals.timeOnSite,\n",
    "  totals.pageviews,\n",
    "  totals.transactions,\n",
    "  totals.transactionRevenue,\n",
    "  trafficSource.source,\n",
    "  trafficSource.medium,\n",
    "  device.deviceCategory,\n",
    "  geoNetwork.country,\n",
    "  hits\n",
    "FROM\n",
    "  `bigquery-public-data.google_analytics_sample.ga_sessions_*`\n",
    "WHERE\n",
    "  _TABLE_SUFFIX BETWEEN '20160801' AND '20170731'\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and load data into a DataFrame\n",
    "logger.info(\"Executing BigQuery and loading data...\")\n",
    "df = client.query(query).to_dataframe()\n",
    "logger.info(f\"Data loaded. Shape: {df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8bed8-50be-4cff-9a48-f38d191788be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning functions\n",
    "def safe_json_loads(x):\n",
    "    try:\n",
    "        return json.loads(x) if isinstance(x, str) else x\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "def flatten_nested_columns(df):\n",
    "    nested_columns = ['totals', 'trafficSource', 'device', 'geoNetwork']\n",
    "    for col in nested_columns:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                flattened = pd.json_normalize(df[col].apply(safe_json_loads))\n",
    "                flattened.columns = [f'{col}_{subcol}' for subcol in flattened.columns]\n",
    "                df = pd.concat([df.drop(col, axis=1), flattened], axis=1)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error flattening column {col}: {str(e)}\")\n",
    "    return df\n",
    "\n",
    "def extract_hit_level_data(df):\n",
    "    try:\n",
    "        hits_data = df.apply(lambda row: pd.json_normalize(safe_json_loads(row['hits'])), axis=1).explode().reset_index()\n",
    "        hits_data = hits_data.rename(columns={'index': 'session_id'})\n",
    "        \n",
    "        hits_data['date'] = df['date'].repeat(df['hits'].apply(lambda x: len(safe_json_loads(x)))).reset_index(drop=True)\n",
    "        hits_data['date'] = pd.to_datetime(hits_data['date'], format='%Y%m%d')\n",
    "        \n",
    "        columns_to_keep = [\n",
    "            'session_id', 'date',\n",
    "            'hitNumber', 'time', 'hour', 'minute',\n",
    "            'isEntrance', 'isExit', 'page.pagePath', 'page.pageTitle',\n",
    "            'eventInfo.eventCategory', 'eventInfo.eventAction', 'eventInfo.eventLabel',\n",
    "            'transaction.transactionId', 'transaction.transactionRevenue',\n",
    "            'item.productName', 'item.productCategory', 'item.productSKU', 'item.itemRevenue'\n",
    "        ]\n",
    "        \n",
    "        hits_df = hits_data[columns_to_keep].copy()\n",
    "        \n",
    "        numeric_columns = ['time', 'hour', 'minute', 'transaction.transactionRevenue', 'item.itemRevenue']\n",
    "        for col in numeric_columns:\n",
    "            if col in hits_df.columns:\n",
    "                hits_df[col] = pd.to_numeric(hits_df[col], errors='coerce')\n",
    "        \n",
    "        boolean_columns = ['isEntrance', 'isExit']\n",
    "        for col in boolean_columns:\n",
    "            if col in hits_df.columns:\n",
    "                hits_df[col] = hits_df[col].astype(bool)\n",
    "        \n",
    "        return hits_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in extract_hit_level_data: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def clean_data(df):\n",
    "    df_cleaned = flatten_nested_columns(df)\n",
    "    logger.info(f\"Flattened DataFrame shape: {df_cleaned.shape}\")\n",
    "\n",
    "    df_cleaned['date'] = pd.to_datetime(df_cleaned['date'], format='%Y%m%d')\n",
    "\n",
    "    if 'geoNetwork_country' in df_cleaned.columns:\n",
    "        df_cleaned['country'] = df_cleaned['geoNetwork_country']\n",
    "\n",
    "    numeric_columns = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "    categorical_columns = df_cleaned.select_dtypes(exclude=[np.number, 'datetime64']).columns\n",
    "\n",
    "    logger.info(f\"Number of numeric columns: {len(numeric_columns)}\")\n",
    "    logger.info(f\"Number of categorical columns: {len(categorical_columns)}\")\n",
    "\n",
    "    for col in df_cleaned.columns:\n",
    "        if col in numeric_columns:\n",
    "            df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
    "        elif col in categorical_columns:\n",
    "            df_cleaned[col] = df_cleaned[col].astype(str)\n",
    "\n",
    "    all_nan_columns = df_cleaned.columns[df_cleaned.isna().all()].tolist()\n",
    "    if all_nan_columns:\n",
    "        logger.warning(f\"Columns with all NaN values: {all_nan_columns}\")\n",
    "        df_cleaned = df_cleaned.drop(columns=all_nan_columns)\n",
    "        numeric_columns = [col for col in numeric_columns if col not in all_nan_columns]\n",
    "        categorical_columns = [col for col in categorical_columns if col not in all_nan_columns]\n",
    "\n",
    "    numeric_imputer = SimpleImputer(strategy='median')\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Shape of numeric data before imputation: {df_cleaned[numeric_columns].shape}\")\n",
    "        imputed_numeric = numeric_imputer.fit_transform(df_cleaned[numeric_columns])\n",
    "        logger.info(f\"Shape of imputed numeric data: {imputed_numeric.shape}\")\n",
    "        df_cleaned[numeric_columns] = imputed_numeric\n",
    "        logger.info(\"Numeric imputation successful\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during numeric imputation: {str(e)}\")\n",
    "        logger.info(\"Numeric columns:\")\n",
    "        logger.info(numeric_columns.tolist())\n",
    "\n",
    "    try:\n",
    "        df_cleaned[categorical_columns] = categorical_imputer.fit_transform(df_cleaned[categorical_columns])\n",
    "        logger.info(\"Categorical imputation successful\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during categorical imputation: {str(e)}\")\n",
    "        logger.info(\"Categorical columns:\")\n",
    "        logger.info(categorical_columns.tolist())\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_cleaned[numeric_columns] = scaler.fit_transform(df_cleaned[numeric_columns])\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    for col in categorical_columns:\n",
    "        df_cleaned[col] = le.fit_transform(df_cleaned[col].astype(str))\n",
    "\n",
    "    logger.info(f\"Final cleaned session-level DataFrame shape: {df_cleaned.shape}\")\n",
    "\n",
    "    hit_level_df = extract_hit_level_data(df)\n",
    "    logger.info(f\"Hit-level DataFrame shape: {hit_level_df.shape}\")\n",
    "\n",
    "    return df_cleaned, hit_level_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce5ac2-b3a2-49f0-a778-af619a82301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def engineer_features(df_cleaned, hit_level_df):\n",
    "    logger.info(\"Starting feature engineering\")\n",
    "    \n",
    "    df_engineered = df_cleaned.copy()\n",
    "    \n",
    "    # Create time-based features\n",
    "    df_engineered['day_of_week'] = df_engineered['date'].dt.dayofweek\n",
    "    df_engineered['is_weekend'] = df_engineered['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df_engineered['month'] = df_engineered['date'].dt.month\n",
    "    df_engineered['quarter'] = df_engineered['date'].dt.quarter\n",
    "    \n",
    "    # Create engagement features\n",
    "    df_engineered['session_duration_seconds'] = df_engineered['totals_timeOnSite']\n",
    "    df_engineered['pageviews_per_session'] = df_engineered['totals_pageviews']\n",
    "    df_engineered['is_bounce'] = (df_engineered['totals_bounces'] > 0).astype(int)\n",
    "    \n",
    "    # Create device type features\n",
    "    df_engineered['is_mobile'] = (df_engineered['device_deviceCategory'] == 'mobile').astype(int)\n",
    "    df_engineered['is_tablet'] = (df_engineered['device_deviceCategory'] == 'tablet').astype(int)\n",
    "    df_engineered['is_desktop'] = (df_engineered['device_deviceCategory'] == 'desktop').astype(int)\n",
    "    \n",
    "    # Create traffic source features\n",
    "    df_engineered['is_organic_search'] = (df_engineered['trafficSource_medium'] == 'organic').astype(int)\n",
    "    df_engineered['is_paid_search'] = (df_engineered['trafficSource_medium'] == 'cpc').astype(int)\n",
    "    df_engineered['is_referral'] = (df_engineered['trafficSource_medium'] == 'referral').astype(int)\n",
    "    \n",
    "    # Create geographical features\n",
    "    df_engineered['is_us'] = (df_engineered['geoNetwork_country'] == 'United States').astype(int)\n",
    "    \n",
    "    # Process hit-level data\n",
    "    logger.info(\"Processing hit-level data\")\n",
    "    hit_level_features = hit_level_df.groupby('session_id').agg({\n",
    "        'time': ['count', 'mean', 'max'],\n",
    "        'isEntrance': 'sum',\n",
    "        'isExit': 'sum',\n",
    "        'eventInfo.eventCategory': 'nunique',\n",
    "        'transaction.transactionId': 'nunique',\n",
    "        'transaction.transactionRevenue': 'sum',\n",
    "        'item.productName': 'nunique',\n",
    "    })\n",
    "    \n",
    "    hit_level_features.columns = [\n",
    "        'total_hits', 'avg_time_per_hit', 'max_time_per_hit',\n",
    "        'num_entrance_pages', 'num_exit_pages', 'num_unique_events',\n",
    "        'num_transactions', 'total_revenue', 'num_unique_products_viewed'\n",
    "    ]\n",
    "    \n",
    "    df_engineered = df_engineered.merge(hit_level_features, left_on='session_id', right_index=True, how='left')\n",
    "    \n",
    "    # Create ratio features\n",
    "    df_engineered['avg_pageviews_per_session'] = df_engineered['pageviews_per_session'] / df_engineered['totals_visits'].replace(0, 1)\n",
    "    df_engineered['conversion_rate'] = df_engineered['num_transactions'] / df_engineered['totals_visits'].replace(0, 1)\n",
    "    df_engineered['avg_revenue_per_session'] = df_engineered['total_revenue'] / df_engineered['totals_visits'].replace(0, 1)\n",
    "    \n",
    "    # Create user segments\n",
    "    df_engineered['user_value_segment'] = pd.qcut(df_engineered['total_revenue'].fillna(0), q=4, labels=['Low', 'Medium', 'High', 'VIP'])\n",
    "    df_engineered['engagement_segment'] = pd.qcut(df_engineered['total_hits'].fillna(0), q=3, labels=['Low', 'Medium', 'High'])\n",
    "    \n",
    "    logger.info(\"Feature engineering completed successfully\")\n",
    "    logger.info(f\"Engineered DataFrame shape: {df_engineered.shape}\")\n",
    "    \n",
    "    return df_engineered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd1938-b40d-456c-b519-5ea12426d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Clean the data\n",
    "    df_cleaned, hit_level_df = clean_data(df)\n",
    "    \n",
    "    # Engineer features\n",
    "    df_engineered = engineer_features(df_cleaned, hit_level_df)\n",
    "    \n",
    "    # Create target variable\n",
    "    df_engineered['made_purchase'] = (df_engineered['totals_transactions'] > 0).astype(int)\n",
    "    \n",
    "    # Select features for modeling\n",
    "    features_for_modeling = [\n",
    "        'session_duration_seconds', 'pageviews_per_session', 'is_bounce',\n",
    "        'is_mobile', 'is_tablet', 'is_desktop',\n",
    "        'is_organic_search', 'is_paid_search', 'is_referral',\n",
    "        'is_us', 'total_hits', 'avg_time_per_hit', 'max_time_per_hit',\n",
    "        'num_entrance_pages', 'num_exit_pages', 'num_unique_events',\n",
    "        'avg_pageviews_per_session', 'conversion_rate', 'avg_revenue_per_session',\n",
    "        'is_weekend', 'month', 'quarter'\n",
    "    ]\n",
    "    \n",
    "    # Prepare final dataset for modeling\n",
    "    X = df_engineered[features_for_modeling]\n",
    "    y = df_engineered['made_purchase']\n",
    "    \n",
    "    logger.info(f\"Final dataset shape: X: {X.shape}, y: {y.shape}\")\n",
    "    logger.info(\"Data preparation completed. Ready for modeling.\")\n",
    "\n",
    "    # Optional: Save the prepared dataset\n",
    "    X.to_csv('prepared_features.csv', index=False)\n",
    "    y.to_csv('target_variable.csv', index=False)\n",
    "    logger.info(\"Prepared dataset saved to CSV files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
